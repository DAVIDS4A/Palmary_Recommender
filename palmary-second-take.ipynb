{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel\nfrom surprise import Dataset, Reader, SVD\nfrom surprise.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.metrics import precision_score, recall_score\n\n# Load a larger and more diverse dataset if available\n# df = pd.read_csv('/path/to/larger/dataset.csv')\n\n# For demonstration purposes, let's use a subset of your original dataset\ndf = pd.read_csv('/kaggle/input/bakery-with-simulatedrating/Bakery_with_SimulatedRating.csv')\n\n# Label encoding for categorical variables\nlabel_encoder = LabelEncoder()\ndf['DateTime'] = label_encoder.fit_transform(df['DateTime'])\ndf['Daypart'] = label_encoder.fit_transform(df['Daypart'])\ndf['DayType'] = label_encoder.fit_transform(df['DayType'])\n\n# Collaborative Filtering (using Surprise's SVD)\nreader = Reader(rating_scale=(1, 5))\ndata = Dataset.load_from_df(df[['TransactionNo', 'Items', 'SimulatedRating']], reader)\ntrainset, testset = train_test_split(data, test_size=0.2)\n\ncf_model = SVD(n_factors=100, n_epochs=20)  # Adjust parameters for SVD\ncf_model.fit(trainset)\ncf_predictions = cf_model.test(testset)\n\n# Calculate MAE and RMSE for Collaborative Filtering\ncf_true_ratings = [testset[i][2] for i in range(len(testset))]\ncf_pred_ratings = [cf_predictions[i].est for i in range(len(cf_predictions))]\n\ncf_mae = mean_absolute_error(cf_true_ratings, cf_pred_ratings)\ncf_rmse = np.sqrt(mean_squared_error(cf_true_ratings, cf_pred_ratings))\n\n#print(f'Collaborative Filtering MAE: {cf_mae}')\n#print(f'Collaborative Filtering RMSE: {cf_rmse}')\n\n# Content-Based Filtering (using scikit-learn)\ntfidf_vectorizer = TfidfVectorizer(stop_words='english')\ntfidf_matrix = tfidf_vectorizer.fit_transform(df['Items'])\n\ncosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n\n# Function to get content-based recommendations for a given item\ndef get_content_based_recommendations(item_name):\n    item_index = df[df['Items'] == item_name].index[0]\n    sim_scores = list(enumerate(cosine_sim[item_index]))\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n    sim_scores = sim_scores[1:6]  # Top 5 similar items (excluding itself)\n    recommended_indices = [i[0] for i in sim_scores]\n    recommended_items = df['Items'].iloc[recommended_indices].tolist()\n    return recommended_items\n\n# Hybrid Recommendation System with precision and recall calculation\ndef hybrid_recommendation(transaction_no):\n    if transaction_no not in df['TransactionNo'].values:\n        return \"Transaction not found.\"\n\n    # Find the items associated with the transaction\n    purchased_items = df[df['TransactionNo'] == transaction_no]['Items'].tolist()\n\n    # Get collaborative filtering predictions for each purchased item\n    cf_predictions = {}\n    for item in purchased_items:\n        cf_predictions[item] = cf_model.predict(transaction_no, item).est\n\n    # Sort items by predicted rating in descending order\n    sorted_cf_items = sorted(cf_predictions.keys(), key=lambda x: cf_predictions[x], reverse=True)\n\n    # Get content-based recommendations for each purchased item\n    content_based_recommendations = {}\n    for item in purchased_items:\n        content_based_recommendations[item] = get_content_based_recommendations(item)\n\n    # Combine recommendations from CF and CBF, with relaxed filtering\n    hybrid_recommendations = []\n    explanations = {}\n    for item in sorted_cf_items:\n        for rec_item in content_based_recommendations.get(item, []):\n            if rec_item not in hybrid_recommendations:\n                hybrid_recommendations.append(rec_item)\n                explanations[rec_item] = content_based_recommendations[item]\n\n    # Calculate precision and recall\n    true_items_set = set(purchased_items)\n    recommended_items_set = set(hybrid_recommendations)\n\n    if len(recommended_items_set) == 0:\n        precision = 0.0\n    else:\n        precision = len(true_items_set.intersection(recommended_items_set)) / len(recommended_items_set)\n    \n    recall = len(true_items_set.intersection(recommended_items_set)) / len(true_items_set)\n\n    #return hybrid_recommendations, explanations, precision, recall\n    return hybrid_recommendations\n\n# Example usage\ntransaction_no = 60  # Replace with the transaction number you want to get recommendations for\n#recommendations, explanations, precision, recall = hybrid_recommendation(transaction_no)\nrecommendations=hybrid_recommendation(transaction_no)\nprint(\"Hybrid Recommendations:\")\nprint(recommendations)\n#print(\"Explanations for Recommendations:\")\n#print(explanations)\n#print(f'Hybrid Recommendation Precision: {precision}')\n#print(f'Hybrid Recommendation Recall: {recall}')\n\n\n\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-23T02:06:04.198888Z","iopub.execute_input":"2023-09-23T02:06:04.199251Z","iopub.status.idle":"2023-09-23T02:06:09.943600Z","shell.execute_reply.started":"2023-09-23T02:06:04.199220Z","shell.execute_reply":"2023-09-23T02:06:09.942530Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Hybrid Recommendations:\n['Coffee', 'Cookies', 'Juice']\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}